{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python hands-on session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By: Ties de Kok  \n",
    "Version: Python 2.7 (see any notes for Python 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. handling files\n",
    "2. data handling\n",
    "3. web scraping\n",
    "4. **text mining**\n",
    "5. (interactive) visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebooks we have either worked with quantitative data (Pandas) or we have extracted information from webpages or documents.  \n",
    "In this notebook we will try to convert qualitative data, more specifically text, into something that we can use for analysis.  \n",
    "\n",
    "Extracting this \"information\" from text is often called \"text mining\" or \"natural language processing\" (NLP)\n",
    "\n",
    "NLP is a sub-field that can easily fill an entire separate workshop, here I will only touch upon the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Basic steps for NLP:\n",
    "1. Obtain and load some raw text\n",
    "2. Process this text (\"clean\" the text)\n",
    "3. Analyse the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: For the sake of consistency I have prepared this notebook with Python 2.7**  \n",
    "**However, when doing text analysis I highly recommend to use Python 3.5 instead because of improved unicode support!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Overview of tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are many different \"NLP\" tools in the Python eco-system.  \n",
    "\n",
    "Several well known options:  \n",
    "1. NLTK (Natural Language Toolkit) (http://www.nltk.org/)\n",
    "2. TextBlob (http://textblob.readthedocs.io/en/dev/#)\n",
    "3. Spacy (https://spacy.io/)\n",
    "\n",
    "**Installation instructions:**\n",
    "1. NLTK: \n",
    "    - `pip install nltk` \n",
    "    - run `nltk.download()` **in the notebook** to download and install the NLTK data\n",
    "2. TextBlob: \n",
    "    - `pip install -U textblob`\n",
    "    - `python -m textblob.download_corpora` **in the console** to download data\n",
    "3. Spacy: \n",
    "    - `conda install -c spacy spacy=0.101.0`\n",
    "    - `python -m spacy.en.download` **in the console** to download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some example text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obviously need some text to extract information from, for this example we will get some text from Twitter.  \n",
    "\n",
    "**Note: You can skip this step by loading the pre-downloaded tweets, this is explained if you scroll down a couple of cells.**\n",
    "\n",
    "To download some twitter data we will use the `tweepy` package. (http://tweepy.readthedocs.io/en/v3.5.0/)  \n",
    "Install `tweepy` using `pip install tweepy`.  \n",
    "\n",
    "Before we can use `tweepy` we need a twitter account that allows us to use the API.  \n",
    "\n",
    "Go to: https://apps.twitter.com and login with your twitter account.  \n",
    "Create a new app and fill something in like:\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/1265025/python_tut/Python_tweepy.PNG)\n",
    "\n",
    "*Note:* you need to have verified your Twitter account with your mobile phone (https://twitter.com/settings/devices)\n",
    "\n",
    "After you have created your app click the `Keys and Access Tokens` tab.  \n",
    "Click `Generate my access token and Token Secret`.\n",
    "\n",
    "You will need 4 things from this page:  \n",
    "1. Consumer Key\n",
    "2. Consumer Secret\n",
    "3. Access Token\n",
    "4. Access Token Secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate ourselfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in your details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = 'YOUR-CONSUMER-KEY'  \n",
    "consumer_secret = 'YOUR-CONSUMER-SECRET'  \n",
    "access_token = 'YOUR-ACCESS-TOKEN'  \n",
    "access_secret = 'YOUR-ACCESS-SECRET'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download 20 most recent tweets from @TheEconomist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many of Russia‚Äôs ultra-rich have struggled to preserve their fortunes amidst market turmoil https://t.co/jtupmMTeKN https://t.co/caGdrpa7cI\n",
      "RT @stevenmazie: This one goes to 11: Trump‚Äôs #SCOTUS wish-list is designed to reassure conservatives (my @TheEconomist post today) https:/‚Ä¶\n",
      "RT @Watchjen: Military exercises this wk near Taiwan were not, Beijing said, \"aimed at any specific target‚Äù. Tsai Ing-wen sworn in https://‚Ä¶\n",
      "RT @EconCulture: Inside a mind losing its grip with \"The Father\" https://t.co/IDI2Skb8yc https://t.co/jZGrs3ucay\n",
      "Our quote of the day is from American novelist Nathaniel Hawthorne https://t.co/pukODOYo9o\n",
      "RT @AdamCommentism: A tiny robot collects a swallowed battery (Melanie Gonick/@MIT). Full story: https://t.co/lMcUFqQlqY üòç Soooo cool https‚Ä¶\n",
      "Breathalysers are relatively new in Nairobi. And Kenyans have figured out a way around them https://t.co/zO7r9cduzp https://t.co/AJUFmCCSWg\n",
      "RT @rachelsllloyd: \"Since when did @TheEconomist write about film?\" Since 1927. https://t.co/t1355zT4H8\n",
      "The musicians who keep a-changin' https://t.co/92NgM3QeHS https://t.co/a2JhrvWU5n\n",
      "\"Virginity tests\" are pointless, degrading, unscientific‚Äîand far too common https://t.co/KlqHrdCkve https://t.co/emdMTzL8i2\n",
      "How gender is trumping religion in American politics https://t.co/p9GjKDA5in https://t.co/0rYouUkKja\n",
      "Investors fear Brexit, but don't think it's likely https://t.co/dzJZld5KK4 https://t.co/h81XDQSrl4\n",
      "RT @Ananyo: Selective reporting of clinical trial data is rife. Bitcoin could help https://t.co/JsoFGcggyb (by me) https://t.co/VjJW2cW8Lu\n",
      "Venezuela's president: the only man who can claim to fight a fictional war‚Äîand then lose it? https://t.co/uR7jRoR2l0 https://t.co/8HR2n9oUxx\n",
      "Politicians can use referendums to avoid making tricky decisions https://t.co/HRA9GEy1Ic https://t.co/oFHtCAfbXX\n",
      "Do economic expansions die of natural causes? Or something more sinister? https://t.co/vtDxK4uUEo https://t.co/HHo4iVcJZT\n",
      "Should referendums be used to make political decisions? https://t.co/Lql64HKvPl\n",
      "A South Korean novel wins the world's biggest translation award https://t.co/U1eIBH6Klg https://t.co/DvdUhV7q1G\n",
      "Wars and terrorism are behind a surge in mental disorders in the Middle East https://t.co/J7Skkmj2tJ https://t.co/pMaekGtZjX\n",
      "Closing Kenya's largest refugee camp won't be easy. And it would provoke outrage https://t.co/xPHKi2hMJ0 https://t.co/xwhFR1drI7\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "for status in tweepy.Cursor(api.user_timeline, id=\"TheEconomist\").items(20):\n",
    "    tweets.append(status.text)\n",
    "    print(status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you cannot or do not want to use `tweepy` you can also load the tweets from a file that I included:  \n",
    "\n",
    "```\n",
    "import pickle\n",
    "tweets = pickle.load(open(\"tweets.p\", \"rb\"))\n",
    "```\n",
    "\n",
    "If you want to follow the examples below it is best to load the data from the file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "tweets = pickle.load(open(\"tweets.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the text (pre-processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, for example, are not interested in the link. So we would like to remove any links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_link(text):\n",
    "    clean = re.sub(r'http(.*)', '', text)\n",
    "    clean = clean.strip()\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Enough time has been wasted issuing warnings about antibiotic resistance. Time to act',\n",
       " u\"Kosovo's recognition by FIFA may open a Pandora's box\",\n",
       " u\"Israel's Dimona nuclear reactor has long been a subject of speculation. It may now close\",\n",
       " u\"National regulation is needed to protect Canada's stockmarket from fraud\",\n",
       " u'Secessionist Catalonia is cracking down on businesses that communicate only in Spanish']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tweets = [remove_link(x) for x in tweets]\n",
    "c_tweets[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic example NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Split text into a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"Kosovo's recognition by FIFA may open a Pandora's box\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(c_tweets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Enough time has been wasted issuing warnings about antibiotic resistance.',\n",
       "  u'Time to act'],\n",
       " [u\"Kosovo's recognition by FIFA may open a Pandora's box\"],\n",
       " [u\"Israel's Dimona nuclear reactor has long been a subject of speculation.\",\n",
       "  u'It may now close']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [nltk.sent_tokenize(x) for x in c_tweets]\n",
    "sentences[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split text into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Enough',\n",
       " u'time',\n",
       " u'has',\n",
       " u'been',\n",
       " u'wasted',\n",
       " u'issuing',\n",
       " u'warnings',\n",
       " u'about',\n",
       " u'antibiotic',\n",
       " u'resistance',\n",
       " u'.',\n",
       " u'Time',\n",
       " u'to',\n",
       " u'act']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [nltk.word_tokenize(x) for x in c_tweets]\n",
    "words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Enough', 'JJ'),\n",
       " (u'time', 'NN'),\n",
       " (u'has', 'VBZ'),\n",
       " (u'been', 'VBN'),\n",
       " (u'wasted', 'VBN'),\n",
       " (u'issuing', 'JJ'),\n",
       " (u'warnings', 'NNS'),\n",
       " (u'about', 'IN'),\n",
       " (u'antibiotic', 'JJ'),\n",
       " (u'resistance', 'NN'),\n",
       " (u'.', '.'),\n",
       " (u'Time', 'NNP'),\n",
       " (u'to', 'TO'),\n",
       " (u'act', 'VB')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = [nltk.pos_tag(x) for x in words]\n",
    "pos[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'warnings'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'warning'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize(words[0][6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do a lot of classification tasks using `NLTK` but they require a training set.  \n",
    "To prevent turning this notebook into a `machine learning` tutorial I will skip this, however the internet is full of resources.  \n",
    "For example:  \n",
    "https://github.com/nltk/nltk/wiki/Sentiment-Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Basic example TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Turn text into a `TextBlob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_tb = [TextBlob(x) for x in c_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Enough time has been wasted issuing warnings about antibiotic resistance. Time to act\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access characteristics of a piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Enough', u'JJ'),\n",
       " (u'time', u'NN'),\n",
       " (u'has', u'VBZ'),\n",
       " (u'been', u'VBN'),\n",
       " (u'wasted', u'VBN'),\n",
       " (u'issuing', u'JJ'),\n",
       " (u'warnings', u'NNS'),\n",
       " (u'about', u'IN'),\n",
       " (u'antibiotic', u'JJ'),\n",
       " (u'resistance', u'NN'),\n",
       " (u'Time', u'NNP'),\n",
       " (u'to', u'TO'),\n",
       " (u'act', u'VB')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tb[0].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList([u'enough', u'antibiotic resistance'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tb[0].noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Enough time has been wasted issuing warnings about antibiotic resistance.\"),\n",
       " Sentence(\"Time to act\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tb[0].sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList([u'Enough', u'time', u'has', u'been', u'wasted', u'issuing', u'warnings', u'about', u'antibiotic', u'resistance', u'Time', u'to', u'act'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tb[0].words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Enough', u'Enough')\n",
      "(u'time', u'time')\n",
      "(u'has', u'ha')\n",
      "(u'been', u'been')\n",
      "(u'wasted', u'wasted')\n",
      "(u'issuing', u'issuing')\n",
      "(u'warnings', u'warning')\n",
      "(u'about', u'about')\n",
      "(u'antibiotic', u'antibiotic')\n",
      "(u'resistance', u'resistance')\n",
      "(u'Time', u'Time')\n",
      "(u'to', u'to')\n",
      "(u'act', u'act')\n"
     ]
    }
   ],
   "source": [
    "for x in text_tb[0].words:\n",
    "    print(x, x.lemmatize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve definition of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Enough', u'an adequate quantity; a quantity that is large enough to achieve a purpose')\n",
      "(u'time', u'an instance or single occasion for some event')\n",
      "(u'has', u'(astronomy) the angular distance of a celestial point measured westward along the celestial equator from the zenith crossing; the right ascension for an observer at a particular location and time of day')\n",
      "(u'been', u'have the quality of being; (copula, used with an adjective or a predicate noun)')\n",
      "(u'wasted', u'spend thoughtlessly; throw away')\n",
      "(u'issuing', u'the act of providing an item for general use or for official purposes (usually in quantity)')\n"
     ]
    }
   ],
   "source": [
    "for x in text_tb[0].words[0:6]:\n",
    "    print(x, x.definitions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "for x in text_tb[0:5]:\n",
    "    print(x.detect_language())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* when it comes to sentiment analysis there really is no one-size-fits all solution.  \n",
    "The below is decent but it is better to use a sentiment resource (e.g. a list with positive / negative words) that is custom to your type of text and goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enough time has been wasted issuing warnings about antibiotic resistance.\n",
      "Sentiment(polarity=-0.1, subjectivity=0.25)\n",
      "Time to act\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Kosovo's recognition by FIFA may open a Pandora's box\n",
      "Sentiment(polarity=0.0, subjectivity=0.5)\n",
      "Israel's Dimona nuclear reactor has long been a subject of speculation.\n",
      "Sentiment(polarity=-0.10833333333333334, subjectivity=0.3666666666666667)\n",
      "It may now close\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "National regulation is needed to protect Canada's stockmarket from fraud\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Secessionist Catalonia is cracking down on businesses that communicate only in Spanish\n",
      "Sentiment(polarity=-0.051851851851851864, subjectivity=0.42962962962962964)\n"
     ]
    }
   ],
   "source": [
    "for text in text_tb[0:5]:\n",
    "    for sentence in text.sentences:\n",
    "        print(sentence)\n",
    "        print(sentence.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Basic example Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* Loading the English file might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_text = [parser(x) for x in c_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kosovo's recognition by FIFA may open a Pandora's box"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access characteristics of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'National', u'national', u'national', -11.179020881652832)\n",
      "(u'regulation', u'regulation', u'regulation', -11.475606918334961)\n",
      "(u'is', u'is', u'be', -4.457748889923096)\n",
      "(u'needed', u'needed', u'need', -9.039335250854492)\n",
      "(u'to', u'to', u'to', -3.8560216426849365)\n",
      "(u'protect', u'protect', u'protect', -10.150605201721191)\n",
      "(u'Canada', u'canada', u'canada', -9.756773948669434)\n",
      "(u\"'s\", u\"'s\", u\"'s\", -4.830559253692627)\n",
      "(u'stockmarket', u'stockmarket', u'stockmarket', -17.218788146972656)\n",
      "(u'from', u'from', u'from', -6.010132312774658)\n",
      "(u'fraud', u'fraud', u'fraud', -11.487196922302246)\n"
     ]
    }
   ],
   "source": [
    "for token in spacy_text[3]:\n",
    "    print(token.orth_, token.lower_, token.lemma_, token.prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note, **prob**:* The unigram log-probability of the word, estimated from counts from a large corpus, smoothed using Simple Good Turing estimation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Kosovo, u'GPE')\n",
      "(FIFA, u'ORG')\n",
      "(Pandora, u'GPE')\n"
     ]
    }
   ],
   "source": [
    "for token in spacy_text[1]:\n",
    "    if token.ent_type_ != \"\":\n",
    "        print(token, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also packages available that are more focussed on particular tasks, such as sentiment analysis.  \n",
    "One that is, for example, easy to use is: `AFINN` --> http://neuro.compute.dtu.dk/wiki/AFINN\n",
    "\n",
    "In essence it is a word list but you can also install it directly by doing `pip install afinn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "afinn = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afinn.score('This is utterly excellent!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-5.0, Enough time has been wasted issuing warnings about antibiotic resistance.)\n",
      "(0.0, Time to act)\n",
      "(2.0, Kosovo's recognition by FIFA may open a Pandora's box)\n",
      "(0.0, Israel's Dimona nuclear reactor has long been a subject of speculation.)\n",
      "(0.0, It may now close)\n",
      "(-3.0, National regulation is needed to protect Canada's stockmarket from fraud)\n",
      "(0.0, Secessionist Catalonia is cracking down on businesses that communicate only in Spanish)\n",
      "(0.0, Our quote of the day is from the Scottish biologist Sir Alexander Fleming)\n",
      "(2.0, RT @EconCulture: \"Where to Invade Next\": a documentary worth watching, despite its cringe-inducing creator)\n",
      "(-1.0, A plane bound for Cairo from Paris has disappeared.)\n",
      "(0.0, The cause is unknown as yet)\n",
      "(2.0, Sir Nicholas Winton, born #onthisday, rescued 669 children from Czechoslovakia in 1938-39)\n",
      "(0.0, Confusion still reigns over what exactly business \"platforms\" are)\n",
      "(-1.0, A plane disappears en route from Paris to Egypt)\n",
      "(0.0, What tribal hunters teach the modern world about sleep #econarchive)\n",
      "(-1.0, Supply shortages have increased the price of oil‚Äîtemporarily)\n",
      "(0.0, Finnish teachers have high social status: only doctors are more sought-after as partners)\n",
      "(0.0, Why America is so unprepared for Zika)\n",
      "(-1.0, Communist kitsch: comic or clueless?)\n",
      "(2.0, Translated literary fiction now sells better than books originally written in English)\n",
      "(-1.0, A row erupts at the UN over plans for a high-level meeting on the fight against HIV/AIDS)\n",
      "(0.0, RT @EconAsia: The BJP‚Äôs Hindu-first Indian nationalism found few fans in Assam.)\n",
      "(0.0, Until now)\n",
      "(0.0, China's rural population may be second-class citizens, but they are at least earning more)\n"
     ]
    }
   ],
   "source": [
    "for text in spacy_text:\n",
    "    for sentence in text.sents:\n",
    "        print(afinn.score(sentence.text), sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Reddit for threads about Egyptian Airline crash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some more text to work with we will extract some text from the website http://www.reddit.com  \n",
    "\n",
    "We could directly use the API and `requests` but it is easier to use a wrapper called `praw`.\n",
    "\n",
    "You can install `praw` by running `pip install praw` and information is available here: https://praw.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = praw.Reddit(user_agent='Python tutorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_news = r.get_subreddit('worldnews').get_top_from_day(limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission_titles = []\n",
    "for submission in new_news:\n",
    "    submission_titles.append(submission.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* you can load the submission titles I am using by loading:  \n",
    "```\n",
    "import pickle\n",
    "submission_titles = pickle.load(open(r'submission_titles.p', 'r'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Oil company records from 1960s reveal patents to reduce CO2 emissions in cars: ExxonMobil and others pursued research into technologies, yet blocked government efforts to fight climate change for more than 50 years, findings show',\n",
       " u'EgyptAir Flight 804 black boxes located',\n",
       " u\"The world's largest cruise ship and its supersized pollution problem: each of the Harmony\\u2019s three four-storey high 16-cylinder W\\xe4rtsil\\xe4 engines will, at full power, burn 1,377 US gallons of fuel an hour, or about 96,000 gallons a day of some of the most polluting diesel fuel in the world\",\n",
       " u\"Women in Iran are cutting their hair short and dressing as men in a bid to bypass state 'morality' police who rigorously enforce penalties for not wearing a hijab.\",\n",
       " u'Poland last week dismissed 32 of 39 scientific experts on the State Council for Nature Conservation after they criticized the logging plan of Bialowieza Forest']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_titles[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Find reddit threads that talk about the Egyptian airplane that went missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use a very simple word-search based approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EgyptAir Flight 804 black boxes located\n",
      "EgyptAir crash: Internal blast 'tore right side' of jet, pilot says\n",
      "EgyptAir: Images released of debris found in plane search\n",
      "EgyptAir Jetliner that crashed into the Mediterranean was once the target of political vandals who wrote in Arabic on its underside, ‚ÄúWe will bring this plane down.‚Äù\n",
      "Egyptian military releases photo of debris collected from Egyptair ms804 crash. Search still underway for blackbox and cvr\n",
      "Egypt Slams ‚ÄòDisrespectful‚Äô CNN Coverage of EgyptAir Tragedy\n",
      "Ominous graffiti was scribbled on the bottom of the crashed EgyptAir flight\n",
      "EgyptAir Flight 804: Black boxes for crashed airplane located\n",
      "The Egyptian Army has shared the first photos of wreckage from the crashed EgyptAir flight\n"
     ]
    }
   ],
   "source": [
    "for x in submission_titles:\n",
    "    if 'egypt' in x.lower():\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the problem is that we also get result that are not related to the plane crash, such as:\n",
    "> Egypt Gets $25 Billion Loan From Russia for Nuclear Plant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can try to add additional keywords such as 'plane' and 'flight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EgyptAir Flight 804 black boxes located\n",
      "EgyptAir: Images released of debris found in plane search\n",
      "EgyptAir Jetliner that crashed into the Mediterranean was once the target of political vandals who wrote in Arabic on its underside, ‚ÄúWe will bring this plane down.‚Äù\n",
      "Ominous graffiti was scribbled on the bottom of the crashed EgyptAir flight\n",
      "EgyptAir Flight 804: Black boxes for crashed airplane located\n",
      "The Egyptian Army has shared the first photos of wreckage from the crashed EgyptAir flight\n"
     ]
    }
   ],
   "source": [
    "for x in submission_titles:\n",
    "    if 'egypt' in x.lower():\n",
    "        if 'plane' in x.lower() or 'flight' in x.lower():\n",
    "            print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this becomes tedious if we want to keep adding additional keywords such as 'aircraft'.  \n",
    "A better approach is to, for example, include synonyms for the word 'airplane'.  \n",
    "\n",
    "We can do this using one of the above libraries but we could also use `PyDictionary`, `pip install PyDictionary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "dictionary=PyDictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'jet', u'aircraft', u'plane', u'cab', u'ship']\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.synonym('airplane'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plane_words = dictionary.synonym('airplane') + ['airplane', 'flight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EgyptAir Flight 804 black boxes located\n",
      "EgyptAir crash: Internal blast 'tore right side' of jet, pilot says\n",
      "EgyptAir: Images released of debris found in plane search\n",
      "EgyptAir Jetliner that crashed into the Mediterranean was once the target of political vandals who wrote in Arabic on its underside, ‚ÄúWe will bring this plane down.‚Äù\n",
      "Ominous graffiti was scribbled on the bottom of the crashed EgyptAir flight\n",
      "EgyptAir Flight 804: Black boxes for crashed airplane located\n",
      "The Egyptian Army has shared the first photos of wreckage from the crashed EgyptAir flight\n"
     ]
    }
   ],
   "source": [
    "for x in submission_titles:\n",
    "    if 'egypt' in x.lower():\n",
    "        if any(word in x.lower() for word in plane_words):\n",
    "            print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning: basic topic classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** I have little experience with machine learning, I base the code below on examples for the sake of illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some data that we can classify:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download the titles of the top posts from two sub-reddits: '/r/Python' and '/r/Java'.  \n",
    "\n",
    "The goal is to build a classifier that can identify a title as being from either '/r/Python' or '/r/Java'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_python = r.get_subreddit('python').get_top_from_year(limit=300)\n",
    "top_python = [x.title for x in top_python]\n",
    "top_python = top_python[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"High Performance Python\" co-author Micha Gorelick has quite an impressive author bio...\n",
      "TIL about \"Google Python Style Guide\"\n",
      "A PEP8 Wallpapper\n",
      "The Idiomatic Way to Merge Dictionaries in Python\n",
      "Django awarded Mozilla Open Source Support Grant\n"
     ]
    }
   ],
   "source": [
    "for x in top_python[0:5]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_java = r.get_subreddit('java').get_top_from_year(limit=300)\n",
    "top_java = [x.title for x in top_java]\n",
    "top_java = top_java[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top Starred repositories in Github have been analysed to understand which are the most common whitespace types in different programming languages\n",
      "JDK 8 Massive Open and Online Course: Lambdas and Streams Introduction\n",
      "Code Academy has a Java Course now\n",
      "Java is the most searched programming language on Google!\n",
      "Improving DuckDuckGo's Java-related searches\n"
     ]
    }
   ],
   "source": [
    "for x in top_java[0:5]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* I use `[50:]` to remove the first 50 titles as they are often not representative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert it into a data structure that we can use for machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "python_tuple = tuple((x, 0, 'python') for x in top_python)\n",
    "java_tuple = tuple((x, 1, 'java') for x in top_java)\n",
    "data = list(python_tuple + java_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import machine learning tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* we will use `sklearn` which comes with `Anaconda`\n",
    "\n",
    "This example is based on: http://nbviewer.jupyter.org/github/gmonce/scikit-learn-book/blob/master/Chapter%202%20-%20Supervised%20Learning%20-%20Text%20Classification%20with%20Naive%20Bayes.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the sample into our training and evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SPLIT_PERC = 0.75\n",
    "split_size = int(len(data)*SPLIT_PERC)\n",
    "\n",
    "data_d = [x[0] for x in data]\n",
    "data_t = [x[1] for x in data]\n",
    "\n",
    "X_train = data_d[:split_size]\n",
    "X_test = data_d[split_size:]\n",
    "y_train = data_t[:split_size]\n",
    "y_test = data_t[split_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification with Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    cv = KFold(len(y), K, shuffle=True, random_state=0)\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    print scores\n",
    "    print (\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n",
    "        np.mean(scores), sem(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial data consists of text, while we need numerical data for the classifier.  \n",
    "In the example below I use `CountVectorizer` : http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.91  0.88  0.91  0.89  0.94]\n",
      "Mean score: 0.906 (+/-0.010)\n"
     ]
    }
   ],
   "source": [
    "evaluate_cross_validation(clf, data_d, data_t, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print \"Accuracy on training set:\"\n",
    "    print clf.score(X_train, y_train)\n",
    "    print \"Accuracy on testing set:\"\n",
    "    print clf.score(X_test, y_test)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print \"Classification Report:\"\n",
    "    print metrics.classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.997333333333\n",
      "Accuracy on testing set:\n",
      "0.888\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.89      0.89        64\n",
      "          1       0.89      0.89      0.89        61\n",
      "\n",
      "avg / total       0.89      0.89      0.89       125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eye-ball the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expected_group = zip(X_test, y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* we code 0 to indicate Python and 1 to indicate Java:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Announcing General Availability of PyCharm 5: Python 3.5, Docker Integration, Thread Concurrency Visualization, and much more', 0, 0)\n",
      "(u'Pyxley: Python Powered Dashboards', 0, 0)\n",
      "(u'Livecoding.tv, Stream yourself as a developer', 1, 1)\n",
      "(u'Decorated Concurrency - Python multiprocessing made really really easy', 0, 0)\n",
      "(u'Go and Quasar: a comparison of style and performance', 1, 0)\n",
      "(u'Hashmap Performance Improvements in Java 8', 1, 1)\n",
      "(u'All the PyCon 2015 talks you should watch', 0, 0)\n",
      "(u'Transcrypt Python to JavaScript compiler moved to Beta', 0, 0)\n",
      "(u'MicroPython 1.8 released', 0, 1)\n",
      "(u'Hey everyone! I made my first Tic Tac Toe game in Python! What do you think?', 0, 0)\n",
      "(u'PEP 0506 -- Adding A Secrets Module To The Standard Library', 0, 0)\n",
      "(u'(Free) Introducing Java 8: A Quick-Start Guide to Lambdas and Streams', 1, 1)\n",
      "(u'matplotlib 1.5.0 is out -- still alive an kicking with pandas DataFrame support and pretty seaborn styles', 0, 0)\n",
      "(u'How do commercial Java apps deal with exposed or decompiled source code? Do they need to?', 1, 1)\n",
      "(u'Magic decorator syntax for asynchronous code in Python', 0, 0)\n",
      "(u'Farewell to Oracle', 1, 1)\n",
      "(u'OpenCV 3.0 released \\u2013 the most functional and the fastest OpenCV ever', 0, 1)\n",
      "(u'How fast are the Java 8 Streams compared to for-loops?', 1, 1)\n",
      "(u'Curio - a new concurrent I/O library by David Beazley', 0, 1)\n",
      "(u'PyPy 5.0 released', 0, 0)\n"
     ]
    }
   ],
   "source": [
    "for text, actual, predict in expected_group[20:40]:\n",
    "    print(text, actual, predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
